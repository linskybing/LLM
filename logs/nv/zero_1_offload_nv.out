+--------------------------------------------------+
| Nvidia hpc sdk v24.11 with cuda v12.6 is loaded. |
+--------------------------------------------------+
--------------------------------
loading CUDA 12.8 with cuDNN / NCCL
based on cuda_12.8.0_570.86.10_linux.run
--------------------------------

CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
Mon Aug 25 16:01:31 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.8     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Tesla V100-SXM2-32GB           On  | 00000000:1B:00.0 Off |                    0 |
| N/A   28C    P0              40W / 300W |      0MiB / 32768MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   1  Tesla V100-SXM2-32GB           On  | 00000000:1C:00.0 Off |                    0 |
| N/A   27C    P0              40W / 300W |      0MiB / 32768MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   2  Tesla V100-SXM2-32GB           On  | 00000000:3D:00.0 Off |                    0 |
| N/A   27C    P0              42W / 300W |      0MiB / 32768MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   3  Tesla V100-SXM2-32GB           On  | 00000000:3E:00.0 Off |                    0 |
| N/A   28C    P0              42W / 300W |      0MiB / 32768MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   4  Tesla V100-SXM2-32GB           On  | 00000000:B1:00.0 Off |                    0 |
| N/A   28C    P0              42W / 300W |      0MiB / 32768MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   5  Tesla V100-SXM2-32GB           On  | 00000000:B2:00.0 Off |                    0 |
| N/A   28C    P0              43W / 300W |      0MiB / 32768MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   6  Tesla V100-SXM2-32GB           On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0              41W / 300W |      0MiB / 32768MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   7  Tesla V100-SXM2-32GB           On  | 00000000:DC:00.0 Off |                    0 |
| N/A   26C    P0              42W / 300W |      0MiB / 32768MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
True 8
[2025-08-25 16:01:37,101] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-25 16:01:37,107] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-25 16:01:37,123] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-25 16:01:37,130] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-25 16:01:37,131] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-25 16:01:37,138] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-25 16:01:37,140] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-25 16:01:37,140] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-25 16:01:41,106] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-25 16:01:41,106] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-25 16:01:41,108] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-25 16:01:41,109] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-25 16:01:41,126] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-25 16:01:41,126] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-25 16:01:41,128] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-25 16:01:41,139] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[nltk_data] Downloading package words to /home/u8644434/nltk_data...
[nltk_data]   Package words is already up-to-date!
[nltk_data] Downloading package words to /home/u8644434/nltk_data...
[nltk_data]   Package words is already up-to-date!
[nltk_data] Downloading package words to /home/u8644434/nltk_data...
[nltk_data]   Package words is already up-to-date!
[nltk_data] Downloading package words to /home/u8644434/nltk_data...
[nltk_data]   Package words is already up-to-date!
[nltk_data] Downloading package words to /home/u8644434/nltk_data...
[nltk_data]   Package words is already up-to-date!
[nltk_data] Downloading package words to /home/u8644434/nltk_data...
[nltk_data] Downloading package words to /home/u8644434/nltk_data...
[nltk_data]   Package words is already up-to-date!
[nltk_data] Downloading package words to /home/u8644434/nltk_data...
[nltk_data]   Package words is already up-to-date!
[nltk_data]   Package words is already up-to-date!
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 42.71it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 41.30it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 42.05it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 42.47it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 41.99it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 42.69it/s]
[2025-08-25 16:01:44,470] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.17.4, git-hash=unknown, git-branch=unknown
[2025-08-25 16:01:44,470] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-08-25 16:01:44,472] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][2025-08-25 16:01:44,523] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.17.4, git-hash=unknown, git-branch=unknown
[2025-08-25 16:01:44,523] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-08-25 16:01:44,524] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 41.10it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 41.96it/s]
[2025-08-25 16:01:44,602] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.17.4, git-hash=unknown, git-branch=unknown
[2025-08-25 16:01:44,602] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.17.4, git-hash=unknown, git-branch=unknown
[2025-08-25 16:01:44,602] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-08-25 16:01:44,602] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-08-25 16:01:44,603] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[2025-08-25 16:01:44,603] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[2025-08-25 16:01:44,629] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.17.4, git-hash=unknown, git-branch=unknown
[2025-08-25 16:01:44,629] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-08-25 16:01:44,630] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[2025-08-25 16:01:44,816] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.17.4, git-hash=unknown, git-branch=unknown
[2025-08-25 16:01:44,816] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-08-25 16:01:44,818] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[2025-08-25 16:01:44,944] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.17.4, git-hash=unknown, git-branch=unknown
[2025-08-25 16:01:44,945] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-08-25 16:01:44,946] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[2025-08-25 16:01:44,959] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.17.4, git-hash=unknown, git-branch=unknown
[2025-08-25 16:01:44,959] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-08-25 16:01:44,960] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[2025-08-25 16:01:54,380] [INFO] [engine.py:1339:_configure_distributed_model] ********** distributed groups summary **********
	 self.dp_world_size=8
	 self.mp_world_size=1
	 self.seq_dp_world_size=8
	 self.sequence_parallel_size=1
***********************************************
[2025-08-25 16:01:56,356] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
[2025-08-25 16:01:57,033] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-08-25 16:01:57,033] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
[2025-08-25 16:01:57,042] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2025-08-25 16:01:57,042] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2025-08-25 16:01:57,042] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 1 optimizer
[2025-08-25 16:01:57,042] [INFO] [stage_1_and_2.py:172:__init__] Reduce bucket size 200000000
[2025-08-25 16:01:57,042] [INFO] [stage_1_and_2.py:173:__init__] Allgather bucket size 500000000
[2025-08-25 16:01:57,042] [INFO] [stage_1_and_2.py:174:__init__] CPU Offload: True
[2025-08-25 16:01:57,042] [INFO] [stage_1_and_2.py:175:__init__] Round robin gradient partitioning: False
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
[2025-08-25 16:02:18,895] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-08-25 16:02:18,896] [INFO] [utils.py:782:see_memory_usage] MA 12.8 GB         Max_MA 12.8 GB         CA 12.81 GB         Max_CA 13 GB 
[2025-08-25 16:02:18,896] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 122.24 GB, percent = 16.2%
[2025-08-25 16:02:22,326] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-08-25 16:02:22,327] [INFO] [utils.py:782:see_memory_usage] MA 12.8 GB         Max_MA 12.8 GB         CA 12.81 GB         Max_CA 13 GB 
[2025-08-25 16:02:22,327] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 127.62 GB, percent = 16.9%
[2025-08-25 16:02:22,327] [INFO] [stage_1_and_2.py:599:__init__] optimizer state initialized
[2025-08-25 16:02:22,502] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-08-25 16:02:22,503] [INFO] [utils.py:782:see_memory_usage] MA 12.8 GB         Max_MA 12.8 GB         CA 12.81 GB         Max_CA 13 GB 
[2025-08-25 16:02:22,503] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 127.79 GB, percent = 16.9%
[2025-08-25 16:02:22,508] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-08-25 16:02:22,508] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-08-25 16:02:22,508] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-08-25 16:02:22,508] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:02:22,509] [INFO] [logging.py:107:log_dist] [Rank 0] [TorchCheckpointEngine] Initialized with serialization = True
[2025-08-25 16:02:22,509] [INFO] [config.py:954:print] DeepSpeedEngine configuration:
[2025-08-25 16:02:22,509] [INFO] [config.py:958:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-08-25 16:02:22,509] [INFO] [config.py:958:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-08-25 16:02:22,509] [INFO] [config.py:958:print]   amp_enabled .................. False
[2025-08-25 16:02:22,509] [INFO] [config.py:958:print]   amp_params ................... False
[2025-08-25 16:02:22,510] [INFO] [config.py:958:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-08-25 16:02:22,510] [INFO] [config.py:958:print]   bfloat16_config .............. enabled=False immediate_grad_update=False check_grad_overflow=False
[2025-08-25 16:02:22,510] [INFO] [config.py:958:print]   checkpoint_config ............ {'tag_validation': 'WARN', 'checkpoint_serialization': True, 'writer': None}
[2025-08-25 16:02:22,510] [INFO] [config.py:958:print]   checkpoint_parallel_write_pipeline  False
[2025-08-25 16:02:22,510] [INFO] [config.py:958:print]   checkpoint_tag_validation_enabled  True
[2025-08-25 16:02:22,510] [INFO] [config.py:958:print]   checkpoint_tag_validation_fail  False
[2025-08-25 16:02:22,510] [INFO] [config.py:958:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1457ed817aa0>
[2025-08-25 16:02:22,510] [INFO] [config.py:958:print]   communication_data_type ...... None
[2025-08-25 16:02:22,510] [INFO] [config.py:958:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False keep_int_input_tensors=True keep_all_input_tensors=False
[2025-08-25 16:02:22,510] [INFO] [config.py:958:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-08-25 16:02:22,510] [INFO] [config.py:958:print]   curriculum_enabled_legacy .... False
[2025-08-25 16:02:22,510] [INFO] [config.py:958:print]   curriculum_params_legacy ..... False
[2025-08-25 16:02:22,510] [INFO] [config.py:958:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-08-25 16:02:22,510] [INFO] [config.py:958:print]   data_efficiency_enabled ...... False
[2025-08-25 16:02:22,510] [INFO] [config.py:958:print]   dataloader_drop_last ......... False
[2025-08-25 16:02:22,510] [INFO] [config.py:958:print]   disable_allgather ............ False
[2025-08-25 16:02:22,510] [INFO] [config.py:958:print]   dump_state ................... False
[2025-08-25 16:02:22,510] [INFO] [config.py:958:print]   eigenvalue_enabled ........... False
[2025-08-25 16:02:22,510] [INFO] [config.py:958:print]   eigenvalue_gas_boundary_resolution  1
[2025-08-25 16:02:22,510] [INFO] [config.py:958:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-08-25 16:02:22,510] [INFO] [config.py:958:print]   eigenvalue_layer_num ......... 0
[2025-08-25 16:02:22,510] [INFO] [config.py:958:print]   eigenvalue_max_iter .......... 100
[2025-08-25 16:02:22,510] [INFO] [config.py:958:print]   eigenvalue_stability ......... 1e-06
[2025-08-25 16:02:22,510] [INFO] [config.py:958:print]   eigenvalue_tol ............... 0.01
[2025-08-25 16:02:22,510] [INFO] [config.py:958:print]   eigenvalue_verbose ........... False
[2025-08-25 16:02:22,510] [INFO] [config.py:958:print]   elasticity_enabled ........... False
[2025-08-25 16:02:22,510] [INFO] [config.py:958:print]   float16_config ............... enabled=True auto_cast=False loss_scale=1024.0 initial_scale_power=10 loss_scale_window=1000 hysteresis=2 consecutive_hysteresis=False min_loss_scale=1 fp16_master_weights_and_grads=False
[2025-08-25 16:02:22,510] [INFO] [config.py:958:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-08-25 16:02:22,510] [INFO] [config.py:958:print]   global_rank .................. 0
[2025-08-25 16:02:22,510] [INFO] [config.py:958:print]   grad_accum_dtype ............. None
[2025-08-25 16:02:22,510] [INFO] [config.py:958:print]   gradient_accumulation_steps .. 1
[2025-08-25 16:02:22,510] [INFO] [config.py:958:print]   gradient_clipping ............ 0.0
[2025-08-25 16:02:22,510] [INFO] [config.py:958:print]   gradient_predivide_factor .... 1.0
[2025-08-25 16:02:22,510] [INFO] [config.py:958:print]   graph_harvesting ............. False
[2025-08-25 16:02:22,510] [INFO] [config.py:958:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-08-25 16:02:22,510] [INFO] [config.py:958:print]   load_universal_checkpoint .... False
[2025-08-25 16:02:22,510] [INFO] [config.py:958:print]   memory_breakdown ............. False
[2025-08-25 16:02:22,510] [INFO] [config.py:958:print]   mics_hierarchial_params_gather  False
[2025-08-25 16:02:22,511] [INFO] [config.py:958:print]   mics_shard_size .............. -1
[2025-08-25 16:02:22,511] [INFO] [config.py:958:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-08-25 16:02:22,511] [INFO] [config.py:958:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-08-25 16:02:22,511] [INFO] [config.py:958:print]   optimizer_legacy_fusion ...... False
[2025-08-25 16:02:22,511] [INFO] [config.py:958:print]   optimizer_name ............... adamw
[2025-08-25 16:02:22,511] [INFO] [config.py:958:print]   optimizer_params ............. {'lr': 1e-05, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0}
[2025-08-25 16:02:22,511] [INFO] [config.py:958:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-08-25 16:02:22,511] [INFO] [config.py:958:print]   pld_enabled .................. False
[2025-08-25 16:02:22,511] [INFO] [config.py:958:print]   pld_params ................... False
[2025-08-25 16:02:22,511] [INFO] [config.py:958:print]   prescale_gradients ........... False
[2025-08-25 16:02:22,511] [INFO] [config.py:958:print]   scheduler_name ............... None
[2025-08-25 16:02:22,511] [INFO] [config.py:958:print]   scheduler_params ............. None
[2025-08-25 16:02:22,511] [INFO] [config.py:958:print]   seq_parallel_communication_data_type  torch.float32
[2025-08-25 16:02:22,511] [INFO] [config.py:958:print]   sparse_attention ............. None
[2025-08-25 16:02:22,511] [INFO] [config.py:958:print]   sparse_gradients_enabled ..... False
[2025-08-25 16:02:22,511] [INFO] [config.py:958:print]   steps_per_print .............. 1
[2025-08-25 16:02:22,511] [INFO] [config.py:958:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-08-25 16:02:22,511] [INFO] [config.py:958:print]   timers_config ................ enabled=True synchronized=True
[2025-08-25 16:02:22,511] [INFO] [config.py:958:print]   torch_autocast_dtype ......... None
[2025-08-25 16:02:22,511] [INFO] [config.py:958:print]   torch_autocast_enabled ....... False
[2025-08-25 16:02:22,511] [INFO] [config.py:958:print]   torch_autocast_lower_precision_safe_modules  None
[2025-08-25 16:02:22,511] [INFO] [config.py:958:print]   train_batch_size ............. 8
[2025-08-25 16:02:22,511] [INFO] [config.py:958:print]   train_micro_batch_size_per_gpu  1
[2025-08-25 16:02:22,511] [INFO] [config.py:958:print]   use_data_before_expert_parallel_  False
[2025-08-25 16:02:22,511] [INFO] [config.py:958:print]   use_node_local_storage ....... False
[2025-08-25 16:02:22,511] [INFO] [config.py:958:print]   wall_clock_breakdown ......... False
[2025-08-25 16:02:22,511] [INFO] [config.py:958:print]   weight_quantization_config ... None
[2025-08-25 16:02:22,511] [INFO] [config.py:958:print]   world_size ................... 8
[2025-08-25 16:02:22,511] [INFO] [config.py:958:print]   zero_allow_untested_optimizer  False
[2025-08-25 16:02:22,511] [INFO] [config.py:958:print]   zero_config .................. stage=1 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=200000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-08-25 16:02:22,511] [INFO] [config.py:958:print]   zero_enabled ................. True
[2025-08-25 16:02:22,511] [INFO] [config.py:958:print]   zero_force_ds_cpu_optimizer .. True
[2025-08-25 16:02:22,511] [INFO] [config.py:958:print]   zero_optimization_stage ...... 1
[2025-08-25 16:02:22,511] [INFO] [config.py:944:print_user_config]   json = {
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 1, 
    "steps_per_print": 1, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 1e-05, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08, 
            "weight_decay": 0
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 1.024000e+03, 
        "initial_scale_power": 10
    }, 
    "zero_optimization": {
        "stage": 1, 
        "overlap_comm": true, 
        "contiguous_gradients": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "prefetch_bucket_size": 2.000000e+08, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "offload_param": {
            "device": "cpu", 
            "pin_memory": true
        }
    }
}
[2025-08-25 16:02:30,959] [INFO] [logging.py:107:log_dist] [Rank 0] step=1, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[Step 1/100] Loss: 5.1940 | Global Tokens/s: 317.83 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:02:38,029] [INFO] [logging.py:107:log_dist] [Rank 0] step=2, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[Step 2/100] Loss: 5.1141 | Global Tokens/s: 413.51 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:02:44,669] [INFO] [logging.py:107:log_dist] [Rank 0] step=3, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:02:45,027] [INFO] [timer.py:264:stop] epoch=0/micro_step=3/global_step=3, RunningAvgSamplesPerSec=1.205402565289523, CurrSamplesPerSec=1.2054023836651324, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 3/100] Loss: 5.0794 | Global Tokens/s: 421.77 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:02:51,309] [INFO] [logging.py:107:log_dist] [Rank 0] step=4, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:02:51,676] [INFO] [timer.py:264:stop] epoch=0/micro_step=4/global_step=4, RunningAvgSamplesPerSec=1.2047399879897365, CurrSamplesPerSec=1.2040779574660587, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 4/100] Loss: 4.9034 | Global Tokens/s: 421.30 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:02:57,952] [INFO] [logging.py:107:log_dist] [Rank 0] step=5, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:02:58,319] [INFO] [timer.py:264:stop] epoch=0/micro_step=5/global_step=5, RunningAvgSamplesPerSec=1.2048738844404348, CurrSamplesPerSec=1.205141585104484, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 5/100] Loss: 4.7972 | Global Tokens/s: 421.67 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:03:04,614] [INFO] [logging.py:107:log_dist] [Rank 0] step=6, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:03:04,967] [INFO] [timer.py:264:stop] epoch=0/micro_step=6/global_step=6, RunningAvgSamplesPerSec=1.204717766120686, CurrSamplesPerSec=1.20424947253337, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 6/100] Loss: 5.1468 | Global Tokens/s: 421.37 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:03:11,247] [INFO] [logging.py:107:log_dist] [Rank 0] step=7, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:03:11,603] [INFO] [timer.py:264:stop] epoch=0/micro_step=7/global_step=7, RunningAvgSamplesPerSec=1.2050570227336752, CurrSamplesPerSec=1.2064157801492683, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 7/100] Loss: 4.9386 | Global Tokens/s: 422.12 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:03:17,879] [INFO] [logging.py:107:log_dist] [Rank 0] step=8, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:03:18,245] [INFO] [timer.py:264:stop] epoch=0/micro_step=8/global_step=8, RunningAvgSamplesPerSec=1.2050993354943564, CurrSamplesPerSec=1.2053107622802177, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 8/100] Loss: 5.1944 | Global Tokens/s: 421.73 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:03:24,527] [INFO] [logging.py:107:log_dist] [Rank 0] step=9, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:03:24,893] [INFO] [timer.py:264:stop] epoch=0/micro_step=9/global_step=9, RunningAvgSamplesPerSec=1.2049819898609682, CurrSamplesPerSec=1.204278214405203, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 9/100] Loss: 4.8247 | Global Tokens/s: 421.37 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:03:31,175] [INFO] [logging.py:107:log_dist] [Rank 0] step=10, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:03:31,532] [INFO] [timer.py:264:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=1.2051013029729698, CurrSamplesPerSec=1.2059369750125348, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 10/100] Loss: 5.0493 | Global Tokens/s: 421.95 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:03:37,826] [INFO] [logging.py:107:log_dist] [Rank 0] step=11, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:03:38,181] [INFO] [timer.py:264:stop] epoch=0/micro_step=11/global_step=11, RunningAvgSamplesPerSec=1.2049949847786081, CurrSamplesPerSec=1.2041449328443161, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 11/100] Loss: 5.0048 | Global Tokens/s: 421.33 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:03:44,470] [INFO] [logging.py:107:log_dist] [Rank 0] step=12, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:03:44,836] [INFO] [timer.py:264:stop] epoch=0/micro_step=12/global_step=12, RunningAvgSamplesPerSec=1.2047935055076726, CurrSamplesPerSec=1.2029830385394389, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 12/100] Loss: 4.8186 | Global Tokens/s: 420.92 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:03:51,111] [INFO] [logging.py:107:log_dist] [Rank 0] step=13, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:03:51,477] [INFO] [timer.py:264:stop] epoch=0/micro_step=13/global_step=13, RunningAvgSamplesPerSec=1.2048577770919071, CurrSamplesPerSec=1.2055006886349842, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 13/100] Loss: 4.9102 | Global Tokens/s: 421.80 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:03:57,757] [INFO] [logging.py:107:log_dist] [Rank 0] step=14, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:03:58,115] [INFO] [timer.py:264:stop] epoch=0/micro_step=14/global_step=14, RunningAvgSamplesPerSec=1.204966946699788, CurrSamplesPerSec=1.2061689375283975, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 14/100] Loss: 4.7969 | Global Tokens/s: 422.04 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:04:04,410] [INFO] [logging.py:107:log_dist] [Rank 0] step=15, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:04:04,767] [INFO] [timer.py:264:stop] epoch=0/micro_step=15/global_step=15, RunningAvgSamplesPerSec=1.2048536779645975, CurrSamplesPerSec=1.2034959312201967, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 15/100] Loss: 4.9588 | Global Tokens/s: 421.10 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:04:11,051] [INFO] [logging.py:107:log_dist] [Rank 0] step=16, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:04:11,418] [INFO] [timer.py:264:stop] epoch=0/micro_step=16/global_step=16, RunningAvgSamplesPerSec=1.204773703150537, CurrSamplesPerSec=1.203734814760335, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 16/100] Loss: 4.9657 | Global Tokens/s: 421.19 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:04:17,697] [INFO] [logging.py:107:log_dist] [Rank 0] step=17, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:04:18,064] [INFO] [timer.py:264:stop] epoch=0/micro_step=17/global_step=17, RunningAvgSamplesPerSec=1.2047622777263938, CurrSamplesPerSec=1.2046021631560824, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 17/100] Loss: 5.2441 | Global Tokens/s: 421.47 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:04:24,339] [INFO] [logging.py:107:log_dist] [Rank 0] step=18, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:04:24,696] [INFO] [timer.py:264:stop] epoch=0/micro_step=18/global_step=18, RunningAvgSamplesPerSec=1.2049052827826225, CurrSamplesPerSec=1.2070542576916714, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 18/100] Loss: 4.8855 | Global Tokens/s: 422.35 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:04:30,987] [INFO] [logging.py:107:log_dist] [Rank 0] step=19, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:04:31,344] [INFO] [timer.py:264:stop] epoch=0/micro_step=19/global_step=19, RunningAvgSamplesPerSec=1.2048760442839779, CurrSamplesPerSec=1.204408239891985, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 19/100] Loss: 5.1501 | Global Tokens/s: 421.42 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:04:37,628] [INFO] [logging.py:107:log_dist] [Rank 0] step=20, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:04:37,995] [INFO] [timer.py:264:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=1.2048109833129788, CurrSamplesPerSec=1.2037058397354543, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 20/100] Loss: 4.6081 | Global Tokens/s: 421.18 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:04:44,281] [INFO] [logging.py:107:log_dist] [Rank 0] step=21, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:04:44,647] [INFO] [timer.py:264:stop] epoch=0/micro_step=21/global_step=21, RunningAvgSamplesPerSec=1.2047393607919223, CurrSamplesPerSec=1.2034514289704186, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 21/100] Loss: 4.9544 | Global Tokens/s: 421.09 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:04:50,937] [INFO] [logging.py:107:log_dist] [Rank 0] step=22, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:04:51,292] [INFO] [timer.py:264:stop] epoch=0/micro_step=22/global_step=22, RunningAvgSamplesPerSec=1.2047385411102463, CurrSamplesPerSec=1.204722785950671, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 22/100] Loss: 5.0897 | Global Tokens/s: 421.53 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:04:57,582] [INFO] [logging.py:107:log_dist] [Rank 0] step=23, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:04:57,939] [INFO] [timer.py:264:stop] epoch=0/micro_step=23/global_step=23, RunningAvgSamplesPerSec=1.2047316264357775, CurrSamplesPerSec=1.204593168232505, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 23/100] Loss: 4.8815 | Global Tokens/s: 421.49 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:05:04,230] [INFO] [logging.py:107:log_dist] [Rank 0] step=24, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:05:04,597] [INFO] [timer.py:264:stop] epoch=0/micro_step=24/global_step=24, RunningAvgSamplesPerSec=1.2047068358768165, CurrSamplesPerSec=1.2041862884595833, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 24/100] Loss: 4.7003 | Global Tokens/s: 421.34 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:05:10,882] [INFO] [logging.py:107:log_dist] [Rank 0] step=25, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:05:11,248] [INFO] [timer.py:264:stop] epoch=0/micro_step=25/global_step=25, RunningAvgSamplesPerSec=1.2046609953538991, CurrSamplesPerSec=1.2036532046226578, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 25/100] Loss: 4.9499 | Global Tokens/s: 421.15 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:05:17,537] [INFO] [logging.py:107:log_dist] [Rank 0] step=26, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:05:17,893] [INFO] [timer.py:264:stop] epoch=0/micro_step=26/global_step=26, RunningAvgSamplesPerSec=1.2046703063418556, CurrSamplesPerSec=1.2048843173287833, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 26/100] Loss: 4.8045 | Global Tokens/s: 421.58 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:05:24,187] [INFO] [logging.py:107:log_dist] [Rank 0] step=27, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:05:24,544] [INFO] [timer.py:264:stop] epoch=0/micro_step=27/global_step=27, RunningAvgSamplesPerSec=1.2046338364887974, CurrSamplesPerSec=1.2037590408518288, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 27/100] Loss: 4.8442 | Global Tokens/s: 421.20 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:05:30,822] [INFO] [logging.py:107:log_dist] [Rank 0] step=28, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:05:31,189] [INFO] [timer.py:264:stop] epoch=0/micro_step=28/global_step=28, RunningAvgSamplesPerSec=1.2046407940411608, CurrSamplesPerSec=1.2048145775266053, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 28/100] Loss: 4.8296 | Global Tokens/s: 421.56 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:05:37,474] [INFO] [logging.py:107:log_dist] [Rank 0] step=29, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:05:37,840] [INFO] [timer.py:264:stop] epoch=0/micro_step=29/global_step=29, RunningAvgSamplesPerSec=1.2046059915590113, CurrSamplesPerSec=1.2037016512108931, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 29/100] Loss: 4.8293 | Global Tokens/s: 421.18 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:05:44,126] [INFO] [logging.py:107:log_dist] [Rank 0] step=30, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:05:44,483] [INFO] [timer.py:264:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=1.2046234902497703, CurrSamplesPerSec=1.2050959656146245, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 30/100] Loss: 4.6856 | Global Tokens/s: 421.66 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:05:50,779] [INFO] [logging.py:107:log_dist] [Rank 0] step=31, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:05:51,136] [INFO] [timer.py:264:stop] epoch=0/micro_step=31/global_step=31, RunningAvgSamplesPerSec=1.2045773538285511, CurrSamplesPerSec=1.203286786313754, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 31/100] Loss: 4.7695 | Global Tokens/s: 421.03 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:05:57,415] [INFO] [logging.py:107:log_dist] [Rank 0] step=32, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:05:57,782] [INFO] [timer.py:264:stop] epoch=0/micro_step=32/global_step=32, RunningAvgSamplesPerSec=1.2045780700783009, CurrSamplesPerSec=1.2045986603092957, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 32/100] Loss: 4.7558 | Global Tokens/s: 421.48 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:06:04,062] [INFO] [logging.py:107:log_dist] [Rank 0] step=33, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:06:04,428] [INFO] [timer.py:264:stop] epoch=0/micro_step=33/global_step=33, RunningAvgSamplesPerSec=1.20458228051398, CurrSamplesPerSec=1.2047084258572731, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 33/100] Loss: 4.5582 | Global Tokens/s: 421.53 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:06:10,717] [INFO] [logging.py:107:log_dist] [Rank 0] step=34, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:06:11,070] [INFO] [timer.py:264:stop] epoch=0/micro_step=34/global_step=34, RunningAvgSamplesPerSec=1.2046072745079175, CurrSamplesPerSec=1.20538242148669, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 34/100] Loss: 5.0452 | Global Tokens/s: 421.76 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:06:17,367] [INFO] [logging.py:107:log_dist] [Rank 0] step=35, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:06:17,723] [INFO] [timer.py:264:stop] epoch=0/micro_step=35/global_step=35, RunningAvgSamplesPerSec=1.2045675869166987, CurrSamplesPerSec=1.203298782343366, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 35/100] Loss: 4.9857 | Global Tokens/s: 421.03 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:06:24,015] [INFO] [logging.py:107:log_dist] [Rank 0] step=36, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:06:24,381] [INFO] [timer.py:264:stop] epoch=0/micro_step=36/global_step=36, RunningAvgSamplesPerSec=1.2045072953605975, CurrSamplesPerSec=1.2025208735730242, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 36/100] Loss: 5.1513 | Global Tokens/s: 420.76 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:06:30,662] [INFO] [logging.py:107:log_dist] [Rank 0] step=37, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:06:31,029] [INFO] [timer.py:264:stop] epoch=0/micro_step=37/global_step=37, RunningAvgSamplesPerSec=1.2044988455225305, CurrSamplesPerSec=1.2042114402855653, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 37/100] Loss: 4.5970 | Global Tokens/s: 421.35 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:06:37,305] [INFO] [logging.py:107:log_dist] [Rank 0] step=38, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:06:37,672] [INFO] [timer.py:264:stop] epoch=0/micro_step=38/global_step=38, RunningAvgSamplesPerSec=1.2045179803407289, CurrSamplesPerSec=1.2051879006433945, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 38/100] Loss: 4.7740 | Global Tokens/s: 421.69 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:06:43,960] [INFO] [logging.py:107:log_dist] [Rank 0] step=39, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:06:44,316] [INFO] [timer.py:264:stop] epoch=0/micro_step=39/global_step=39, RunningAvgSamplesPerSec=1.2045321967921794, CurrSamplesPerSec=1.205044031120834, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 39/100] Loss: 5.1589 | Global Tokens/s: 421.64 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:06:50,597] [INFO] [logging.py:107:log_dist] [Rank 0] step=40, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:06:50,963] [INFO] [timer.py:264:stop] epoch=0/micro_step=40/global_step=40, RunningAvgSamplesPerSec=1.204526608816254, CurrSamplesPerSec=1.2043197088506763, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 40/100] Loss: 4.7950 | Global Tokens/s: 421.39 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:06:57,247] [INFO] [logging.py:107:log_dist] [Rank 0] step=41, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:06:57,613] [INFO] [timer.py:264:stop] epoch=0/micro_step=41/global_step=41, RunningAvgSamplesPerSec=1.2045127882521889, CurrSamplesPerSec=1.20398766052536, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 41/100] Loss: 4.7008 | Global Tokens/s: 421.27 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:07:03,890] [INFO] [logging.py:107:log_dist] [Rank 0] step=42, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:07:04,257] [INFO] [timer.py:264:stop] epoch=0/micro_step=42/global_step=42, RunningAvgSamplesPerSec=1.2045256692633175, CurrSamplesPerSec=1.2050280621638456, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 42/100] Loss: 4.4801 | Global Tokens/s: 421.64 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:07:10,542] [INFO] [logging.py:107:log_dist] [Rank 0] step=43, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:07:10,898] [INFO] [timer.py:264:stop] epoch=0/micro_step=43/global_step=43, RunningAvgSamplesPerSec=1.2045473044027306, CurrSamplesPerSec=1.2054131661146172, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 43/100] Loss: 5.1254 | Global Tokens/s: 421.77 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:07:17,186] [INFO] [logging.py:107:log_dist] [Rank 0] step=44, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:07:17,542] [INFO] [timer.py:264:stop] epoch=0/micro_step=44/global_step=44, RunningAvgSamplesPerSec=1.2045574072862053, CurrSamplesPerSec=1.2049715899792786, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 44/100] Loss: 4.8542 | Global Tokens/s: 421.62 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:07:23,828] [INFO] [logging.py:107:log_dist] [Rank 0] step=45, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:07:24,195] [INFO] [timer.py:264:stop] epoch=0/micro_step=45/global_step=45, RunningAvgSamplesPerSec=1.2045313854220954, CurrSamplesPerSec=1.2034393004119748, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 45/100] Loss: 4.7205 | Global Tokens/s: 421.08 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:07:30,473] [INFO] [logging.py:107:log_dist] [Rank 0] step=46, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:07:30,839] [INFO] [timer.py:264:stop] epoch=0/micro_step=46/global_step=46, RunningAvgSamplesPerSec=1.2045397897315142, CurrSamplesPerSec=1.2049011045412916, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 46/100] Loss: 4.7387 | Global Tokens/s: 421.59 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:07:37,135] [INFO] [logging.py:107:log_dist] [Rank 0] step=47, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:07:37,491] [INFO] [timer.py:264:stop] epoch=0/micro_step=47/global_step=47, RunningAvgSamplesPerSec=1.2045178205962834, CurrSamplesPerSec=1.2035517903022204, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 47/100] Loss: 4.4964 | Global Tokens/s: 421.12 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:07:43,775] [INFO] [logging.py:107:log_dist] [Rank 0] step=48, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:07:44,132] [INFO] [timer.py:264:stop] epoch=0/micro_step=48/global_step=48, RunningAvgSamplesPerSec=1.204541184885641, CurrSamplesPerSec=1.2055933351727453, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 48/100] Loss: 4.7895 | Global Tokens/s: 421.83 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:07:50,414] [INFO] [logging.py:107:log_dist] [Rank 0] step=49, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:07:50,781] [INFO] [timer.py:264:stop] epoch=0/micro_step=49/global_step=49, RunningAvgSamplesPerSec=1.2045318277717716, CurrSamplesPerSec=1.2041013763963455, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 49/100] Loss: 4.6994 | Global Tokens/s: 421.31 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:07:57,068] [INFO] [logging.py:107:log_dist] [Rank 0] step=50, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:07:57,430] [INFO] [timer.py:264:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=1.2045223742303992, CurrSamplesPerSec=1.2040780438811012, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 50/100] Loss: 4.8953 | Global Tokens/s: 421.31 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:08:03,722] [INFO] [logging.py:107:log_dist] [Rank 0] step=51, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:08:04,078] [INFO] [timer.py:264:stop] epoch=0/micro_step=51/global_step=51, RunningAvgSamplesPerSec=1.2045165213659137, CurrSamplesPerSec=1.204235469471808, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 51/100] Loss: 4.9039 | Global Tokens/s: 421.36 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:08:10,359] [INFO] [logging.py:107:log_dist] [Rank 0] step=52, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:08:10,714] [INFO] [timer.py:264:stop] epoch=0/micro_step=52/global_step=52, RunningAvgSamplesPerSec=1.204552883620296, CurrSamplesPerSec=1.206337145560024, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 52/100] Loss: 5.1061 | Global Tokens/s: 422.09 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:08:16,997] [INFO] [logging.py:107:log_dist] [Rank 0] step=53, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:08:17,364] [INFO] [timer.py:264:stop] epoch=0/micro_step=53/global_step=53, RunningAvgSamplesPerSec=1.204541095547195, CurrSamplesPerSec=1.2039518047317972, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 53/100] Loss: 5.0030 | Global Tokens/s: 421.26 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:08:23,652] [INFO] [logging.py:107:log_dist] [Rank 0] step=54, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:08:24,019] [INFO] [timer.py:264:stop] epoch=0/micro_step=54/global_step=54, RunningAvgSamplesPerSec=1.2045119057392502, CurrSamplesPerSec=1.2030249182301156, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 54/100] Loss: 4.8291 | Global Tokens/s: 420.94 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:08:30,300] [INFO] [logging.py:107:log_dist] [Rank 0] step=55, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:08:30,657] [INFO] [timer.py:264:stop] epoch=0/micro_step=55/global_step=55, RunningAvgSamplesPerSec=1.2045415946798674, CurrSamplesPerSec=1.2060872571246681, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 55/100] Loss: 4.5716 | Global Tokens/s: 422.01 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:08:36,936] [INFO] [logging.py:107:log_dist] [Rank 0] step=56, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:08:37,293] [INFO] [timer.py:264:stop] epoch=0/micro_step=56/global_step=56, RunningAvgSamplesPerSec=1.2045755835869016, CurrSamplesPerSec=1.2063795627246967, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 56/100] Loss: 4.6555 | Global Tokens/s: 422.11 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:08:43,577] [INFO] [logging.py:107:log_dist] [Rank 0] step=57, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:08:43,944] [INFO] [timer.py:264:stop] epoch=0/micro_step=57/global_step=57, RunningAvgSamplesPerSec=1.2045601269479473, CurrSamplesPerSec=1.203725875967508, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 57/100] Loss: 4.7112 | Global Tokens/s: 421.17 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:08:50,228] [INFO] [logging.py:107:log_dist] [Rank 0] step=58, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:08:50,594] [INFO] [timer.py:264:stop] epoch=0/micro_step=58/global_step=58, RunningAvgSamplesPerSec=1.2045491715557581, CurrSamplesPerSec=1.2039467505326387, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 58/100] Loss: 4.7770 | Global Tokens/s: 421.26 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:08:56,878] [INFO] [logging.py:107:log_dist] [Rank 0] step=59, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:08:57,235] [INFO] [timer.py:264:stop] epoch=0/micro_step=59/global_step=59, RunningAvgSamplesPerSec=1.204566659234653, CurrSamplesPerSec=1.2055465986503928, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 59/100] Loss: 4.8002 | Global Tokens/s: 421.81 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:09:03,525] [INFO] [logging.py:107:log_dist] [Rank 0] step=60, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:09:03,882] [INFO] [timer.py:264:stop] epoch=0/micro_step=60/global_step=60, RunningAvgSamplesPerSec=1.2045666228590741, CurrSamplesPerSec=1.2045643680827764, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 60/100] Loss: 4.8570 | Global Tokens/s: 421.47 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:09:10,159] [INFO] [logging.py:107:log_dist] [Rank 0] step=61, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:09:10,526] [INFO] [timer.py:264:stop] epoch=0/micro_step=61/global_step=61, RunningAvgSamplesPerSec=1.204572047301978, CurrSamplesPerSec=1.2048865671342428, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 61/100] Loss: 4.6277 | Global Tokens/s: 421.58 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:09:16,809] [INFO] [logging.py:107:log_dist] [Rank 0] step=62, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:09:17,175] [INFO] [timer.py:264:stop] epoch=0/micro_step=62/global_step=62, RunningAvgSamplesPerSec=1.204565038815278, CurrSamplesPerSec=1.2041515011535378, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 62/100] Loss: 4.7408 | Global Tokens/s: 421.32 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:09:23,460] [INFO] [logging.py:107:log_dist] [Rank 0] step=63, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:09:23,816] [INFO] [timer.py:264:stop] epoch=0/micro_step=63/global_step=63, RunningAvgSamplesPerSec=1.2045825492151045, CurrSamplesPerSec=1.2056339239527103, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 63/100] Loss: 4.5303 | Global Tokens/s: 421.85 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:09:30,112] [INFO] [logging.py:107:log_dist] [Rank 0] step=64, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:09:30,469] [INFO] [timer.py:264:stop] epoch=0/micro_step=64/global_step=64, RunningAvgSamplesPerSec=1.2045621488062248, CurrSamplesPerSec=1.2033188481793404, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 64/100] Loss: 4.6335 | Global Tokens/s: 421.03 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:09:36,745] [INFO] [logging.py:107:log_dist] [Rank 0] step=65, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:09:37,112] [INFO] [timer.py:264:stop] epoch=0/micro_step=65/global_step=65, RunningAvgSamplesPerSec=1.204572524650281, CurrSamplesPerSec=1.2052159947001102, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 65/100] Loss: 4.9690 | Global Tokens/s: 421.69 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:09:43,394] [INFO] [logging.py:107:log_dist] [Rank 0] step=66, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:09:43,760] [INFO] [timer.py:264:stop] epoch=0/micro_step=66/global_step=66, RunningAvgSamplesPerSec=1.2045692642512429, CurrSamplesPerSec=1.2043637133761549, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 66/100] Loss: 4.7473 | Global Tokens/s: 421.40 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:09:50,045] [INFO] [logging.py:107:log_dist] [Rank 0] step=67, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:09:50,402] [INFO] [timer.py:264:stop] epoch=0/micro_step=67/global_step=67, RunningAvgSamplesPerSec=1.204581471459928, CurrSamplesPerSec=1.2053630661662718, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 67/100] Loss: 4.6928 | Global Tokens/s: 421.75 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:09:56,693] [INFO] [logging.py:107:log_dist] [Rank 0] step=68, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:09:57,054] [INFO] [timer.py:264:stop] epoch=0/micro_step=68/global_step=68, RunningAvgSamplesPerSec=1.2045645881108467, CurrSamplesPerSec=1.203468003623533, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 68/100] Loss: 4.7346 | Global Tokens/s: 421.08 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:10:03,340] [INFO] [logging.py:107:log_dist] [Rank 0] step=69, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:10:03,707] [INFO] [timer.py:264:stop] epoch=0/micro_step=69/global_step=69, RunningAvgSamplesPerSec=1.2045478742361884, CurrSamplesPerSec=1.2034456020523983, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 69/100] Loss: 4.9949 | Global Tokens/s: 421.08 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:10:09,993] [INFO] [logging.py:107:log_dist] [Rank 0] step=70, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:10:10,359] [INFO] [timer.py:264:stop] epoch=0/micro_step=70/global_step=70, RunningAvgSamplesPerSec=1.2045321089542382, CurrSamplesPerSec=1.203476593271811, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 70/100] Loss: 4.6997 | Global Tokens/s: 421.09 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:10:16,647] [INFO] [logging.py:107:log_dist] [Rank 0] step=71, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:10:17,006] [INFO] [timer.py:264:stop] epoch=0/micro_step=71/global_step=71, RunningAvgSamplesPerSec=1.2045310613038573, CurrSamplesPerSec=1.2044596440126545, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 71/100] Loss: 4.8188 | Global Tokens/s: 421.43 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:10:23,287] [INFO] [logging.py:107:log_dist] [Rank 0] step=72, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:10:23,643] [INFO] [timer.py:264:stop] epoch=0/micro_step=72/global_step=72, RunningAvgSamplesPerSec=1.2045576455287195, CurrSamplesPerSec=1.20639461329455, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 72/100] Loss: 4.9117 | Global Tokens/s: 422.11 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:10:29,922] [INFO] [logging.py:107:log_dist] [Rank 0] step=73, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:10:30,289] [INFO] [timer.py:264:stop] epoch=0/micro_step=73/global_step=73, RunningAvgSamplesPerSec=1.2045576721177351, CurrSamplesPerSec=1.2045593519813012, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 73/100] Loss: 4.6362 | Global Tokens/s: 421.47 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:10:36,576] [INFO] [logging.py:107:log_dist] [Rank 0] step=74, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:10:36,943] [INFO] [timer.py:264:stop] epoch=0/micro_step=74/global_step=74, RunningAvgSamplesPerSec=1.2045401047551612, CurrSamplesPerSec=1.2032939493818027, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 74/100] Loss: 4.8778 | Global Tokens/s: 421.03 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:10:43,231] [INFO] [logging.py:107:log_dist] [Rank 0] step=75, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:10:43,587] [INFO] [timer.py:264:stop] epoch=0/micro_step=75/global_step=75, RunningAvgSamplesPerSec=1.2045462587696574, CurrSamplesPerSec=1.204989331627936, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 75/100] Loss: 4.7551 | Global Tokens/s: 421.61 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:10:49,880] [INFO] [logging.py:107:log_dist] [Rank 0] step=76, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:10:50,238] [INFO] [timer.py:264:stop] epoch=0/micro_step=76/global_step=76, RunningAvgSamplesPerSec=1.204534341856764, CurrSamplesPerSec=1.203664862536243, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 76/100] Loss: 4.6809 | Global Tokens/s: 421.15 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:10:56,518] [INFO] [logging.py:107:log_dist] [Rank 0] step=77, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:10:56,885] [INFO] [timer.py:264:stop] epoch=0/micro_step=77/global_step=77, RunningAvgSamplesPerSec=1.2045358354245776, CurrSamplesPerSec=1.2046461883255017, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 77/100] Loss: 4.6139 | Global Tokens/s: 421.49 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:11:03,162] [INFO] [logging.py:107:log_dist] [Rank 0] step=78, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:11:03,529] [INFO] [timer.py:264:stop] epoch=0/micro_step=78/global_step=78, RunningAvgSamplesPerSec=1.204541880028416, CurrSamplesPerSec=1.2049952167780051, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 78/100] Loss: 4.7435 | Global Tokens/s: 421.62 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:11:09,812] [INFO] [logging.py:107:log_dist] [Rank 0] step=79, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:11:10,169] [INFO] [timer.py:264:stop] epoch=0/micro_step=79/global_step=79, RunningAvgSamplesPerSec=1.2045557690555808, CurrSamplesPerSec=1.2056120914411175, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 79/100] Loss: 4.7750 | Global Tokens/s: 421.84 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:11:16,466] [INFO] [logging.py:107:log_dist] [Rank 0] step=80, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:11:16,827] [INFO] [timer.py:264:stop] epoch=0/micro_step=80/global_step=80, RunningAvgSamplesPerSec=1.2045290922461505, CurrSamplesPerSec=1.2024783394829133, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 80/100] Loss: 5.0601 | Global Tokens/s: 420.74 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:11:23,113] [INFO] [logging.py:107:log_dist] [Rank 0] step=81, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:11:23,480] [INFO] [timer.py:264:stop] epoch=0/micro_step=81/global_step=81, RunningAvgSamplesPerSec=1.2045190426035983, CurrSamplesPerSec=1.2037355056870471, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 81/100] Loss: 4.7995 | Global Tokens/s: 421.18 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:11:29,756] [INFO] [logging.py:107:log_dist] [Rank 0] step=82, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:11:30,124] [INFO] [timer.py:264:stop] epoch=0/micro_step=82/global_step=82, RunningAvgSamplesPerSec=1.2045255844054992, CurrSamplesPerSec=1.2050424298784128, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 82/100] Loss: 4.7571 | Global Tokens/s: 421.64 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:11:36,405] [INFO] [logging.py:107:log_dist] [Rank 0] step=83, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:11:36,763] [INFO] [timer.py:264:stop] epoch=0/micro_step=83/global_step=83, RunningAvgSamplesPerSec=1.2045432302378671, CurrSamplesPerSec=1.205956392111199, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 83/100] Loss: 4.7323 | Global Tokens/s: 421.95 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:11:43,051] [INFO] [logging.py:107:log_dist] [Rank 0] step=84, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:11:43,409] [INFO] [timer.py:264:stop] epoch=0/micro_step=84/global_step=84, RunningAvgSamplesPerSec=1.2045449308868963, CurrSamplesPerSec=1.2046825180005474, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 84/100] Loss: 4.7590 | Global Tokens/s: 421.51 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:11:49,691] [INFO] [logging.py:107:log_dist] [Rank 0] step=85, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:11:50,057] [INFO] [timer.py:264:stop] epoch=0/micro_step=85/global_step=85, RunningAvgSamplesPerSec=1.204540634772245, CurrSamplesPerSec=1.2041882763663525, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 85/100] Loss: 5.0222 | Global Tokens/s: 421.32 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:11:56,332] [INFO] [logging.py:107:log_dist] [Rank 0] step=86, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:11:56,698] [INFO] [timer.py:264:stop] epoch=0/micro_step=86/global_step=86, RunningAvgSamplesPerSec=1.204553178857606, CurrSamplesPerSec=1.2055950678289602, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 86/100] Loss: 5.0076 | Global Tokens/s: 421.81 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:12:02,980] [INFO] [logging.py:107:log_dist] [Rank 0] step=87, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:12:03,337] [INFO] [timer.py:264:stop] epoch=0/micro_step=87/global_step=87, RunningAvgSamplesPerSec=1.2045698652467738, CurrSamplesPerSec=1.205972992492673, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 87/100] Loss: 4.3044 | Global Tokens/s: 421.97 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:12:09,632] [INFO] [logging.py:107:log_dist] [Rank 0] step=88, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:12:09,987] [INFO] [timer.py:264:stop] epoch=0/micro_step=88/global_step=88, RunningAvgSamplesPerSec=1.204560243829698, CurrSamplesPerSec=1.203742803648877, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 88/100] Loss: 4.9195 | Global Tokens/s: 421.18 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:12:16,271] [INFO] [logging.py:107:log_dist] [Rank 0] step=89, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:12:16,638] [INFO] [timer.py:264:stop] epoch=0/micro_step=89/global_step=89, RunningAvgSamplesPerSec=1.2045525336416967, CurrSamplesPerSec=1.2038896453504362, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 89/100] Loss: 4.8054 | Global Tokens/s: 421.23 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:12:22,922] [INFO] [logging.py:107:log_dist] [Rank 0] step=90, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:12:23,289] [INFO] [timer.py:264:stop] epoch=0/micro_step=90/global_step=90, RunningAvgSamplesPerSec=1.2045432824084479, CurrSamplesPerSec=1.2037387875997607, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 90/100] Loss: 4.6050 | Global Tokens/s: 421.18 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:12:29,567] [INFO] [logging.py:107:log_dist] [Rank 0] step=91, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:12:29,933] [INFO] [timer.py:264:stop] epoch=0/micro_step=91/global_step=91, RunningAvgSamplesPerSec=1.2045481719649218, CurrSamplesPerSec=1.2049784269429864, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 91/100] Loss: 5.0454 | Global Tokens/s: 421.61 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:12:36,224] [INFO] [logging.py:107:log_dist] [Rank 0] step=92, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:12:36,580] [INFO] [timer.py:264:stop] epoch=0/micro_step=92/global_step=92, RunningAvgSamplesPerSec=1.2045466074696851, CurrSamplesPerSec=1.2044072023434726, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 92/100] Loss: 4.5627 | Global Tokens/s: 421.41 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:12:42,864] [INFO] [logging.py:107:log_dist] [Rank 0] step=93, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:12:43,231] [INFO] [timer.py:264:stop] epoch=0/micro_step=93/global_step=93, RunningAvgSamplesPerSec=1.2045386259279456, CurrSamplesPerSec=1.2038205389107668, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 93/100] Loss: 4.8972 | Global Tokens/s: 421.21 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:12:49,516] [INFO] [logging.py:107:log_dist] [Rank 0] step=94, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:12:49,883] [INFO] [timer.py:264:stop] epoch=0/micro_step=94/global_step=94, RunningAvgSamplesPerSec=1.2045284266436758, CurrSamplesPerSec=1.2036008331525077, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 94/100] Loss: 4.8455 | Global Tokens/s: 421.13 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:12:56,168] [INFO] [logging.py:107:log_dist] [Rank 0] step=95, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:12:56,534] [INFO] [timer.py:264:stop] epoch=0/micro_step=95/global_step=95, RunningAvgSamplesPerSec=1.204518581697775, CurrSamplesPerSec=1.2036133535346194, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 95/100] Loss: 4.6327 | Global Tokens/s: 421.14 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:13:02,831] [INFO] [logging.py:107:log_dist] [Rank 0] step=96, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:13:03,186] [INFO] [timer.py:264:stop] epoch=0/micro_step=96/global_step=96, RunningAvgSamplesPerSec=1.2045087163829, CurrSamplesPerSec=1.2035917668315161, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 96/100] Loss: 4.6432 | Global Tokens/s: 421.13 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:13:09,466] [INFO] [logging.py:107:log_dist] [Rank 0] step=97, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:13:09,822] [INFO] [timer.py:264:stop] epoch=0/micro_step=97/global_step=97, RunningAvgSamplesPerSec=1.2045303538430807, CurrSamplesPerSec=1.2065675699910612, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 97/100] Loss: 4.7213 | Global Tokens/s: 422.17 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:13:16,104] [INFO] [logging.py:107:log_dist] [Rank 0] step=98, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:13:16,470] [INFO] [timer.py:264:stop] epoch=0/micro_step=98/global_step=98, RunningAvgSamplesPerSec=1.2045268270062124, CurrSamplesPerSec=1.204191690395467, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 98/100] Loss: 4.5900 | Global Tokens/s: 421.34 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:13:22,751] [INFO] [logging.py:107:log_dist] [Rank 0] step=99, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:13:23,118] [INFO] [timer.py:264:stop] epoch=0/micro_step=99/global_step=99, RunningAvgSamplesPerSec=1.2045256120042092, CurrSamplesPerSec=1.2044088018981758, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 99/100] Loss: 4.7977 | Global Tokens/s: 421.42 | GPU Mem (GB): 12.81 | Peak Mem: 15.95
[2025-08-25 16:13:29,405] [INFO] [logging.py:107:log_dist] [Rank 0] step=100, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]
[2025-08-25 16:13:29,761] [INFO] [timer.py:264:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=1.2045320765760597, CurrSamplesPerSec=1.205159288474288, MemAllocated=13.0GB, MaxMemAllocated=15.95GB
[Step 100/100] Loss: 4.6037 | Global Tokens/s: 421.68 | GPU Mem (GB): 12.81 | Peak Mem: 15.95

Training done in 667.25 seconds.
Avg Global Tokens/s: 420.01
Peak GPU Mem (GB): 15.95
