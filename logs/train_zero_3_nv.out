--------------------------------
loading CUDA 12.8 with cuDNN / NCCL
based on cuda_12.8.0_570.86.10_linux.run
--------------------------------

CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
Sat Aug 16 15:58:13 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.8     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Tesla V100-SXM2-32GB           On  | 00000000:1B:00.0 Off |                    0 |
| N/A   28C    P0              43W / 300W |      0MiB / 32768MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   1  Tesla V100-SXM2-32GB           On  | 00000000:1C:00.0 Off |                    0 |
| N/A   26C    P0              41W / 300W |      0MiB / 32768MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   2  Tesla V100-SXM2-32GB           On  | 00000000:3D:00.0 Off |                    0 |
| N/A   25C    P0              40W / 300W |      0MiB / 32768MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   3  Tesla V100-SXM2-32GB           On  | 00000000:3E:00.0 Off |                    0 |
| N/A   26C    P0              41W / 300W |      0MiB / 32768MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   4  Tesla V100-SXM2-32GB           On  | 00000000:B1:00.0 Off |                    0 |
| N/A   25C    P0              41W / 300W |      0MiB / 32768MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   5  Tesla V100-SXM2-32GB           On  | 00000000:B2:00.0 Off |                    0 |
| N/A   28C    P0              40W / 300W |      0MiB / 32768MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   6  Tesla V100-SXM2-32GB           On  | 00000000:DB:00.0 Off |                    0 |
| N/A   26C    P0              41W / 300W |      0MiB / 32768MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   7  Tesla V100-SXM2-32GB           On  | 00000000:DC:00.0 Off |                    0 |
| N/A   25C    P0              40W / 300W |      0MiB / 32768MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
True 8
[2025-08-16 15:58:32,327] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-16 15:58:55,526] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-16 15:58:58,473] [WARNING] [runner.py:220:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3,4,5,6,7: setting --include=localhost:0,1,2,3,4,5,6,7
[2025-08-16 15:58:58,474] [INFO] [runner.py:610:main] cmd = /home/u8644434/miniconda3/envs/deepspeed/bin/python3.12 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /work/u8644434/LLM/run/quantize.py --deepspeed_config /work/u8644434/LLM/configs/RQ.json --batch_size 1 --seq_len 350 --total_steps 100
[2025-08-16 15:59:00,127] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-16 15:59:03,423] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-16 15:59:04,424] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2025-08-16 15:59:04,425] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=8, node_rank=0
[2025-08-16 15:59:04,425] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2025-08-16 15:59:04,425] [INFO] [launch.py:164:main] dist_world_size=8
[2025-08-16 15:59:04,425] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2025-08-16 15:59:04,426] [INFO] [launch.py:256:main] process 1988803 spawned with command: ['/home/u8644434/miniconda3/envs/deepspeed/bin/python3.12', '-u', '/work/u8644434/LLM/run/quantize.py', '--local_rank=0', '--deepspeed_config', '/work/u8644434/LLM/configs/RQ.json', '--batch_size', '1', '--seq_len', '350', '--total_steps', '100']
[2025-08-16 15:59:04,427] [INFO] [launch.py:256:main] process 1988804 spawned with command: ['/home/u8644434/miniconda3/envs/deepspeed/bin/python3.12', '-u', '/work/u8644434/LLM/run/quantize.py', '--local_rank=1', '--deepspeed_config', '/work/u8644434/LLM/configs/RQ.json', '--batch_size', '1', '--seq_len', '350', '--total_steps', '100']
[2025-08-16 15:59:04,428] [INFO] [launch.py:256:main] process 1988805 spawned with command: ['/home/u8644434/miniconda3/envs/deepspeed/bin/python3.12', '-u', '/work/u8644434/LLM/run/quantize.py', '--local_rank=2', '--deepspeed_config', '/work/u8644434/LLM/configs/RQ.json', '--batch_size', '1', '--seq_len', '350', '--total_steps', '100']
[2025-08-16 15:59:04,429] [INFO] [launch.py:256:main] process 1988806 spawned with command: ['/home/u8644434/miniconda3/envs/deepspeed/bin/python3.12', '-u', '/work/u8644434/LLM/run/quantize.py', '--local_rank=3', '--deepspeed_config', '/work/u8644434/LLM/configs/RQ.json', '--batch_size', '1', '--seq_len', '350', '--total_steps', '100']
[2025-08-16 15:59:04,430] [INFO] [launch.py:256:main] process 1988807 spawned with command: ['/home/u8644434/miniconda3/envs/deepspeed/bin/python3.12', '-u', '/work/u8644434/LLM/run/quantize.py', '--local_rank=4', '--deepspeed_config', '/work/u8644434/LLM/configs/RQ.json', '--batch_size', '1', '--seq_len', '350', '--total_steps', '100']
[2025-08-16 15:59:04,431] [INFO] [launch.py:256:main] process 1988808 spawned with command: ['/home/u8644434/miniconda3/envs/deepspeed/bin/python3.12', '-u', '/work/u8644434/LLM/run/quantize.py', '--local_rank=5', '--deepspeed_config', '/work/u8644434/LLM/configs/RQ.json', '--batch_size', '1', '--seq_len', '350', '--total_steps', '100']
[2025-08-16 15:59:04,434] [INFO] [launch.py:256:main] process 1988809 spawned with command: ['/home/u8644434/miniconda3/envs/deepspeed/bin/python3.12', '-u', '/work/u8644434/LLM/run/quantize.py', '--local_rank=6', '--deepspeed_config', '/work/u8644434/LLM/configs/RQ.json', '--batch_size', '1', '--seq_len', '350', '--total_steps', '100']
[2025-08-16 15:59:04,443] [INFO] [launch.py:256:main] process 1988810 spawned with command: ['/home/u8644434/miniconda3/envs/deepspeed/bin/python3.12', '-u', '/work/u8644434/LLM/run/quantize.py', '--local_rank=7', '--deepspeed_config', '/work/u8644434/LLM/configs/RQ.json', '--batch_size', '1', '--seq_len', '350', '--total_steps', '100']
[2025-08-16 15:59:07,003] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-16 15:59:07,069] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-16 15:59:07,108] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-16 15:59:07,125] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-16 15:59:07,133] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-16 15:59:07,155] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-16 15:59:07,173] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-16 15:59:07,212] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-16 15:59:10,903] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-16 15:59:11,374] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-16 15:59:11,477] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-16 15:59:11,488] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-16 15:59:11,583] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-16 15:59:11,660] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-16 15:59:11,714] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-16 15:59:11,936] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[nltk_data] Downloading package words to /home/u8644434/nltk_data...
[nltk_data] Downloading package words to /home/u8644434/nltk_data...
[nltk_data] Downloading package words to /home/u8644434/nltk_data...
[nltk_data] Downloading package words to /home/u8644434/nltk_data...
[nltk_data] Downloading package words to /home/u8644434/nltk_data...
[nltk_data]   Package words is already up-to-date![nltk_data]   Package words is already up-to-date!

[nltk_data]   Package words is already up-to-date!
[nltk_data]   Package words is already up-to-date!
[nltk_data]   Package words is already up-to-date!
[nltk_data] Downloading package words to /home/u8644434/nltk_data...
[nltk_data]   Package words is already up-to-date!
[nltk_data] Downloading package words to /home/u8644434/nltk_data...
[nltk_data]   Package words is already up-to-date!
[nltk_data] Downloading package words to /home/u8644434/nltk_data...
[nltk_data]   Package words is already up-to-date!
Traceback (most recent call last):
Traceback (most recent call last):
  File "/work/u8644434/LLM/run/quantize.py", line 135, in <module>
Traceback (most recent call last):
Traceback (most recent call last):
  File "/work/u8644434/LLM/run/quantize.py", line 135, in <module>
  File "/work/u8644434/LLM/run/quantize.py", line 135, in <module>
  File "/work/u8644434/LLM/run/quantize.py", line 135, in <module>
    main()
    main()    
main()      File "/work/u8644434/LLM/run/quantize.py", line 71, in main

main()  File "/work/u8644434/LLM/run/quantize.py", line 71, in main

  File "/work/u8644434/LLM/run/quantize.py", line 71, in main
  File "/work/u8644434/LLM/run/quantize.py", line 71, in main
    model = AutoModelForCausalLM.from_pretrained(
        model = AutoModelForCausalLM.from_pretrained(     
 model = AutoModelForCausalLM.from_pretrained(     
  model = AutoModelForCausalLM.from_pretrained(
                ^   ^   ^   ^   ^   ^   ^  ^^  ^^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py", line 600, in from_pretrained
^^^^^^^^^^^^^
^^

  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py", line 600, in from_pretrained
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py", line 600, in from_pretrained
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py", line 600, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^    ^return model_class.from_pretrained(^
        ^return model_class.from_pretrained(return model_class.from_pretrained(^ 

^  ^   ^   ^   ^   ^   ^   ^   ^   ^ ^ ^ ^ ^ ^ ^^^^^^^^
^^^^^^  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 317, in _wrapper
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 317, in _wrapper
^
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 317, in _wrapper
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 317, in _wrapper
    return func(*args, **kwargs)        
return func(*args, **kwargs)return func(*args, **kwargs)    

return func(*args, **kwargs)
                                 ^   ^   ^   ^ ^^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 4994, in from_pretrained
^^
^  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 4994, in from_pretrained

  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 4994, in from_pretrained
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 4994, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
    model = cls(config, *model_args, **model_kwargs)        
model = cls(config, *model_args, **model_kwargs)model = cls(config, *model_args, **model_kwargs)

                               ^   ^   ^   ^   ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^^  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py", line 414, in __init__
^^^^^^^^
^^^
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py", line 414, in __init__

  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py", line 414, in __init__
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py", line 414, in __init__
    super().__init__(config)
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2228, in __init__
    super().__init__(config)
        super().__init__(config)super().__init__(config)

  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2228, in __init__
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2228, in __init__
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2228, in __init__
    self.config._attn_implementation_internal = self._check_and_adjust_attn_implementation(
                                                ^^^^^^^^^^^^^^^^^^^^^^    ^self.config._attn_implementation_internal = self._check_and_adjust_attn_implementation(^
^^         ^self.config._attn_implementation_internal = self._check_and_adjust_attn_implementation( self.config._attn_implementation_internal = self._check_and_adjust_attn_implementation(^
 
^ ^   ^   ^   ^   ^   ^   ^   ^   ^   ^   ^   ^   ^   ^   
        File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2760, in _check_and_adjust_attn_implementation
                                                                                     ^  ^  ^  ^ ^^^^^^^^^^^^^^^^^^^^^^    ^^^attn_implementation = self.get_correct_attn_implementation(applicable_attn_implementation, is_init_check)^^^
^^^^^^^^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^^^^
^^^^^^  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2760, in _check_and_adjust_attn_implementation
^^^
^^
^  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2760, in _check_and_adjust_attn_implementation
^  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2760, in _check_and_adjust_attn_implementation
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2792, in get_correct_attn_implementation
    attn_implementation = self.get_correct_attn_implementation(applicable_attn_implementation, is_init_check)
         attn_implementation = self.get_correct_attn_implementation(applicable_attn_implementation, is_init_check)     
attn_implementation = self.get_correct_attn_implementation(applicable_attn_implementation, is_init_check) 
                                                 ^  ^  ^  ^  ^      ^  self._flash_attn_2_can_dispatch(is_init_check)^  
^  ^  ^  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2489, in _flash_attn_2_can_dispatch
  ^ ^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^    ^^^raise ImportError(f"{preface} the package flash_attn seems to be not installed. {install_message}")^^^
^^^^^^^^^^^ImportError^^^: ^^^FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.^^^^
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2792, in get_correct_attn_implementation
^^^^^^^^
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2792, in get_correct_attn_implementation
    self._flash_attn_2_can_dispatch(is_init_check)
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2489, in _flash_attn_2_can_dispatch
    self._flash_attn_2_can_dispatch(is_init_check)
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2489, in _flash_attn_2_can_dispatch
^^^^^^^^^^^
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2792, in get_correct_attn_implementation
    raise ImportError(f"{preface} the package flash_attn seems to be not installed. {install_message}")
ImportError: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.
    Traceback (most recent call last):
raise ImportError(f"{preface} the package flash_attn seems to be not installed. {install_message}")
  File "/work/u8644434/LLM/run/quantize.py", line 135, in <module>
ImportError: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.
    main()
  File "/work/u8644434/LLM/run/quantize.py", line 71, in main
    self._flash_attn_2_can_dispatch(is_init_check)
    model = AutoModelForCausalLM.from_pretrained(
   File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2489, in _flash_attn_2_can_dispatch
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py", line 600, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^    ^raise ImportError(f"{preface} the package flash_attn seems to be not installed. {install_message}")^
^^^^^^^^^ImportError^: ^FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.^^
^^^^^^
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 317, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 4994, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py", line 414, in __init__
    super().__init__(config)
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2228, in __init__
    self.config._attn_implementation_internal = self._check_and_adjust_attn_implementation(
                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2760, in _check_and_adjust_attn_implementation
    attn_implementation = self.get_correct_attn_implementation(applicable_attn_implementation, is_init_check)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2792, in get_correct_attn_implementation
    self._flash_attn_2_can_dispatch(is_init_check)
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2489, in _flash_attn_2_can_dispatch
    raise ImportError(f"{preface} the package flash_attn seems to be not installed. {install_message}")
ImportError: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.
Traceback (most recent call last):
  File "/work/u8644434/LLM/run/quantize.py", line 135, in <module>
    main()
  File "/work/u8644434/LLM/run/quantize.py", line 71, in main
    model = AutoModelForCausalLM.from_pretrained(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^Traceback (most recent call last):
  File "/work/u8644434/LLM/run/quantize.py", line 135, in <module>
    main()
  File "/work/u8644434/LLM/run/quantize.py", line 71, in main
    model = AutoModelForCausalLM.from_pretrained(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py", line 600, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 317, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 4994, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py", line 414, in __init__
    super().__init__(config)
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2228, in __init__
    self.config._attn_implementation_internal = self._check_and_adjust_attn_implementation(
                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2760, in _check_and_adjust_attn_implementation
    attn_implementation = self.get_correct_attn_implementation(applicable_attn_implementation, is_init_check)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2792, in get_correct_attn_implementation
    self._flash_attn_2_can_dispatch(is_init_check)
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2489, in _flash_attn_2_can_dispatch
    raise ImportError(f"{preface} the package flash_attn seems to be not installed. {install_message}")
ImportError: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.
^^
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py", line 600, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 317, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 4994, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py", line 414, in __init__
    super().__init__(config)
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2228, in __init__
    self.config._attn_implementation_internal = self._check_and_adjust_attn_implementation(
                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2760, in _check_and_adjust_attn_implementation
    attn_implementation = self.get_correct_attn_implementation(applicable_attn_implementation, is_init_check)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2792, in get_correct_attn_implementation
    self._flash_attn_2_can_dispatch(is_init_check)
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2489, in _flash_attn_2_can_dispatch
    raise ImportError(f"{preface} the package flash_attn seems to be not installed. {install_message}")
ImportError: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.
Traceback (most recent call last):
  File "/work/u8644434/LLM/run/quantize.py", line 135, in <module>
    main()
  File "/work/u8644434/LLM/run/quantize.py", line 71, in main
    model = AutoModelForCausalLM.from_pretrained(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py", line 600, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 317, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 4994, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py", line 414, in __init__
    super().__init__(config)
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2228, in __init__
    self.config._attn_implementation_internal = self._check_and_adjust_attn_implementation(
                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2760, in _check_and_adjust_attn_implementation
    attn_implementation = self.get_correct_attn_implementation(applicable_attn_implementation, is_init_check)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2792, in get_correct_attn_implementation
    self._flash_attn_2_can_dispatch(is_init_check)
  File "/home/u8644434/miniconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2489, in _flash_attn_2_can_dispatch
    raise ImportError(f"{preface} the package flash_attn seems to be not installed. {install_message}")
ImportError: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.
[2025-08-16 15:59:20,448] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 1988803
[2025-08-16 15:59:20,469] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 1988804
[2025-08-16 15:59:20,479] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 1988805
[2025-08-16 15:59:20,489] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 1988806
[2025-08-16 15:59:20,489] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 1988807
[2025-08-16 15:59:20,499] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 1988808
[2025-08-16 15:59:20,508] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 1988809
[2025-08-16 15:59:20,517] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 1988810
[2025-08-16 15:59:20,526] [ERROR] [launch.py:325:sigkill_handler] ['/home/u8644434/miniconda3/envs/deepspeed/bin/python3.12', '-u', '/work/u8644434/LLM/run/quantize.py', '--local_rank=7', '--deepspeed_config', '/work/u8644434/LLM/configs/RQ.json', '--batch_size', '1', '--seq_len', '350', '--total_steps', '100'] exits with return code = 1
