bash: /home/sky/miniconda3/envs/deepspeed/lib/libtinfo.so.6: no version information available (required by bash)
bash: /home/sky/miniconda3/envs/deepspeed/lib/libtinfo.so.6: no version information available (required by bash)
[2025-08-25 08:08:31,067] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-25 08:08:31,098] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-25 08:08:31,129] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-25 08:08:31,161] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-25 08:08:32,886] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-25 08:08:32,965] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[nltk_data] Downloading package words to /home/sky/nltk_data...
[nltk_data]   Package words is already up-to-date!
[nltk_data] Downloading package words to /home/sky/nltk_data...
[nltk_data]   Package words is already up-to-date!
/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/transformers/modeling_utils.py:6124: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:36.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/transformers/modeling_utils.py:6124: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:36.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.81s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.86s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.27s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.29s/it]
[2025-08-25 08:08:41,482] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.17.5+a54c3943, git-hash=a54c3943, git-branch=master
[2025-08-25 08:08:41,482] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-08-25 08:08:41,483] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 2
[2025-08-25 08:08:41,521] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.17.5+a54c3943, git-hash=a54c3943, git-branch=master
[2025-08-25 08:08:41,521] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-08-25 08:08:41,522] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 2
[2025-08-25 08:08:41,539] [INFO] [engine.py:1343:_configure_distributed_model] ********** distributed groups summary **********
	 self.dp_world_size=2
	 self.mp_world_size=1
	 self.seq_dp_world_size=2
	 self.sequence_parallel_size=1
***********************************************
[2025-08-25 08:08:42,153] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-08-25 08:08:42,427] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-08-25 08:08:42,427] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-08-25 08:08:42,434] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-08-25 08:08:42,434] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-08-25 08:08:42,434] [INFO] [logging.py:107:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2025-08-25 08:08:42,434] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer
[2025-08-25 08:08:42,437] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 2
[2025-08-25 08:08:42,593] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning
[2025-08-25 08:08:42,593] [INFO] [utils.py:782:see_memory_usage] MA 12.55 GB         Max_MA 12.55 GB         CA 24.86 GB         Max_CA 25 GB 
[2025-08-25 08:08:42,594] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 6.93 GB, percent = 2.8%
[2025-08-25 08:08:42,595] [INFO] [stage3.py:186:__init__] Reduce bucket size 20000000
[2025-08-25 08:08:42,595] [INFO] [stage3.py:187:__init__] Prefetch bucket size 50000000
[2025-08-25 08:08:42,743] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2025-08-25 08:08:42,743] [INFO] [utils.py:782:see_memory_usage] MA 12.55 GB         Max_MA 12.55 GB         CA 24.86 GB         Max_CA 25 GB 
[2025-08-25 08:08:42,743] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 6.93 GB, percent = 2.8%
[2025-08-25 08:08:42,746] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 2
Parameter Offload - Persistent parameters statistics: param_count = 65, numel = 266240
/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[2025-08-25 08:08:43,354] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2025-08-25 08:08:43,354] [INFO] [utils.py:782:see_memory_usage] MA 6.28 GB         Max_MA 12.67 GB         CA 25.17 GB         Max_CA 25 GB 
[2025-08-25 08:08:43,354] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 6.99 GB, percent = 2.8%
[2025-08-25 08:08:43,529] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions
[2025-08-25 08:08:43,529] [INFO] [utils.py:782:see_memory_usage] MA 6.28 GB         Max_MA 6.28 GB         CA 25.17 GB         Max_CA 25 GB 
[2025-08-25 08:08:43,529] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 7.0 GB, percent = 2.8%
/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[2025-08-25 08:08:45,385] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 55
[2025-08-25 08:08:45,386] [INFO] [utils.py:782:see_memory_usage] MA 6.28 GB         Max_MA 6.28 GB         CA 6.28 GB         Max_CA 25 GB 
[2025-08-25 08:08:45,386] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 7.12 GB, percent = 2.9%
[2025-08-25 08:08:45,549] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions
[2025-08-25 08:08:45,549] [INFO] [utils.py:782:see_memory_usage] MA 6.28 GB         Max_MA 6.28 GB         CA 6.28 GB         Max_CA 6 GB 
[2025-08-25 08:08:45,550] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 7.12 GB, percent = 2.9%
[2025-08-25 08:08:45,720] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions
[2025-08-25 08:08:45,720] [INFO] [utils.py:782:see_memory_usage] MA 18.89 GB         Max_MA 19.06 GB         CA 19.31 GB         Max_CA 19 GB 
[2025-08-25 08:08:45,720] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 7.12 GB, percent = 2.9%
[2025-08-25 08:08:45,885] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-08-25 08:08:45,885] [INFO] [utils.py:782:see_memory_usage] MA 18.89 GB         Max_MA 18.89 GB         CA 19.31 GB         Max_CA 19 GB 
[2025-08-25 08:08:45,885] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 7.12 GB, percent = 2.9%
[2025-08-25 08:08:46,049] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-08-25 08:08:46,050] [INFO] [utils.py:782:see_memory_usage] MA 18.89 GB         Max_MA 19.22 GB         CA 19.64 GB         Max_CA 20 GB 
[2025-08-25 08:08:46,050] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 7.12 GB, percent = 2.9%
[2025-08-25 08:08:46,050] [INFO] [stage3.py:554:_setup_for_real_optimizer] optimizer state initialized
[2025-08-25 08:08:46,365] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-08-25 08:08:46,366] [INFO] [utils.py:782:see_memory_usage] MA 25.21 GB         Max_MA 25.69 GB         CA 26.49 GB         Max_CA 26 GB 
[2025-08-25 08:08:46,366] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 7.16 GB, percent = 2.9%
[2025-08-25 08:08:46,366] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3
[2025-08-25 08:08:46,366] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-08-25 08:08:46,366] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-08-25 08:08:46,366] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05], mom=[[0.8, 0.999]]
[2025-08-25 08:08:46,367] [INFO] [logging.py:107:log_dist] [Rank 0] [TorchCheckpointEngine] Initialized with serialization = True
[2025-08-25 08:08:46,367] [INFO] [config.py:954:print] DeepSpeedEngine configuration:
[2025-08-25 08:08:46,367] [INFO] [config.py:958:print]   activation_checkpointing_config  {
    "partition_activations": true, 
    "contiguous_memory_optimization": true, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": true, 
    "profile": false
}
[2025-08-25 08:08:46,367] [INFO] [config.py:958:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-08-25 08:08:46,367] [INFO] [config.py:958:print]   amp_enabled .................. False
[2025-08-25 08:08:46,367] [INFO] [config.py:958:print]   amp_params ................... False
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   bfloat16_config .............. enabled=False immediate_grad_update=False check_grad_overflow=False
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   checkpoint_config ............ {'tag_validation': 'WARN', 'checkpoint_serialization': True, 'writer': None}
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   checkpoint_parallel_write_pipeline  False
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   checkpoint_tag_validation_enabled  True
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   checkpoint_tag_validation_fail  False
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f910ad44bd0>
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   communication_data_type ...... None
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False keep_int_input_tensors=True keep_all_input_tensors=False
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   curriculum_enabled_legacy .... False
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   curriculum_params_legacy ..... False
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   data_efficiency_enabled ...... False
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   dataloader_drop_last ......... False
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   disable_allgather ............ False
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   dump_state ................... False
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   eigenvalue_enabled ........... False
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   eigenvalue_gas_boundary_resolution  1
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   eigenvalue_layer_num ......... 0
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   eigenvalue_max_iter .......... 100
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   eigenvalue_stability ......... 1e-06
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   eigenvalue_tol ............... 0.01
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   eigenvalue_verbose ........... False
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   elasticity_enabled ........... False
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   float16_config ............... enabled=True auto_cast=False loss_scale=0.0 initial_scale_power=16 loss_scale_window=1000 hysteresis=2 consecutive_hysteresis=False min_loss_scale=1 fp16_master_weights_and_grads=False
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   global_rank .................. 0
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   grad_accum_dtype ............. None
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   gradient_accumulation_steps .. 1
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   gradient_clipping ............ 0.0
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   gradient_predivide_factor .... 1.0
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   graph_harvesting ............. False
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   load_universal_checkpoint .... False
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   memory_breakdown ............. False
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   mics_hierarchial_params_gather  False
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   mics_shard_size .............. -1
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   optimizer_legacy_fusion ...... False
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   optimizer_name ............... adamw
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   optimizer_params ............. {'lr': 1e-05, 'betas': [0.8, 0.999], 'eps': 1e-08, 'weight_decay': 3e-07}
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   pld_enabled .................. False
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   pld_params ................... False
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   prescale_gradients ........... False
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   scheduler_name ............... None
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   scheduler_params ............. None
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   seq_parallel_communication_data_type  torch.float32
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   sparse_attention ............. None
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   sparse_gradients_enabled ..... False
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   steps_per_print .............. 1
[2025-08-25 08:08:46,368] [INFO] [config.py:958:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-08-25 08:08:46,369] [INFO] [config.py:958:print]   timers_config ................ enabled=True synchronized=True
[2025-08-25 08:08:46,369] [INFO] [config.py:958:print]   torch_autocast_dtype ......... None
[2025-08-25 08:08:46,369] [INFO] [config.py:958:print]   torch_autocast_enabled ....... False
[2025-08-25 08:08:46,369] [INFO] [config.py:958:print]   torch_autocast_lower_precision_safe_modules  None
[2025-08-25 08:08:46,369] [INFO] [config.py:958:print]   train_batch_size ............. 2
[2025-08-25 08:08:46,369] [INFO] [config.py:958:print]   train_micro_batch_size_per_gpu  1
[2025-08-25 08:08:46,369] [INFO] [config.py:958:print]   use_data_before_expert_parallel_  False
[2025-08-25 08:08:46,369] [INFO] [config.py:958:print]   use_node_local_storage ....... False
[2025-08-25 08:08:46,369] [INFO] [config.py:958:print]   wall_clock_breakdown ......... False
[2025-08-25 08:08:46,369] [INFO] [config.py:958:print]   weight_quantization_config ... None
[2025-08-25 08:08:46,369] [INFO] [config.py:958:print]   world_size ................... 2
[2025-08-25 08:08:46,369] [INFO] [config.py:958:print]   zero_allow_untested_optimizer  False
[2025-08-25 08:08:46,369] [INFO] [config.py:958:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=20000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=20000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=50000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=True zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-08-25 08:08:46,369] [INFO] [config.py:958:print]   zero_enabled ................. True
[2025-08-25 08:08:46,369] [INFO] [config.py:958:print]   zero_force_ds_cpu_optimizer .. True
[2025-08-25 08:08:46,369] [INFO] [config.py:958:print]   zero_optimization_stage ...... 3
[2025-08-25 08:08:46,369] [INFO] [config.py:944:print_user_config]   json = {
    "train_batch_size": 2, 
    "train_micro_batch_size_per_gpu": 1, 
    "steps_per_print": 1, 
    "gradient_accumulation_steps": 1, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 1e-05, 
            "betas": [0.8, 0.999], 
            "eps": 1e-08, 
            "weight_decay": 3e-07
        }
    }, 
    "fp16": {
        "enabled": true
    }, 
    "zero_optimization": {
        "stage": 3, 
        "reduce_scatter": true, 
        "overlap_comm": true, 
        "round_robin_gradients": true, 
        "contiguous_gradients": true, 
        "reduce_bucket_size": 2.000000e+07, 
        "allgather_bucket_size": 2.000000e+07, 
        "allgather_partitions": true, 
        "stage3_prefetch_bucket_size": 5.000000e+07, 
        "stage3_param_persistence_threshold": 1.000000e+05, 
        "sub_group_size": 5.000000e+07, 
        "ignore_unused_parameters": true, 
        "log_trace_cache_warnings": false
    }, 
    "activation_checkpointing": {
        "partition_activations": true, 
        "contiguous_memory_optimization": true, 
        "cpu_checkpointing": false, 
        "number_checkpoints": null, 
        "synchronize_checkpoint_boundary": true, 
        "profile": false
    }, 
    "gradient_checkpointing": {
        "enable": true
    }
}
[2025-08-25 08:08:48,383] [INFO] [loss_scaler.py:191:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
[2025-08-25 08:08:48,383] [INFO] [stage3.py:2038:_loco_err_buf_update] update loco-zero++ error buffer with overflow: True
[2025-08-25 08:08:48,384] [INFO] [logging.py:107:log_dist] [Rank 0] step=1, skipped=1, lr=[1e-05], mom=[[0.8, 0.999]]
[Step 1/100] Loss: 5.1550 | Global Tokens/s: 347.48 | GPU Mem (GB): 25.21 | Peak Mem: 40.08
[2025-08-25 08:08:50,368] [WARNING] [stage3.py:2160:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2025-08-25 08:08:50,369] [INFO] [logging.py:107:log_dist] [Rank 0] step=2, skipped=1, lr=[1e-05], mom=[[0.8, 0.999]]
[Step 2/100] Loss: 5.2009 | Global Tokens/s: 353.13 | GPU Mem (GB): 50.46 | Peak Mem: 50.98
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/sky/LLM/run/native.py", line 154, in <module>
[rank0]:     main()
[rank0]:   File "/home/sky/LLM/run/native.py", line 123, in main
[rank0]:     loss_val = train_step(ds_engine, inputs, labels)
[rank0]:                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/sky/LLM/run/native.py", line 43, in train_step
[rank0]:     outputs = ds_engine(inputs, labels=labels)
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 2109, in forward
[rank0]:     loss = self.module(*inputs, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1879, in _call_impl
[rank0]:     return inner()
[rank0]:            ^^^^^^^
[rank0]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1827, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/transformers/utils/generic.py", line 959, in wrapper
[rank0]:     output = func(self, *args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 460, in forward
[rank0]:     outputs: BaseModelOutputWithPast = self.model(
[rank0]:                                        ^^^^^^^^^^^
[rank0]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1879, in _call_impl
[rank0]:     return inner()
[rank0]:            ^^^^^^^
[rank0]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1827, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/transformers/utils/generic.py", line 1083, in wrapper
[rank0]:     outputs = func(self, *args, **kwargs)
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 390, in forward
[rank0]:     hidden_states = decoder_layer(
[rank0]:                     ^^^^^^^^^^^^^^
[rank0]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/transformers/modeling_layers.py", line 94, in __call__
[rank0]:     return super().__call__(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/sit[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/sky/LLM/run/native.py", line 154, in <module>
[rank1]:     main()
[rank1]:   File "/home/sky/LLM/run/native.py", line 123, in main
[rank1]:     loss_val = train_step(ds_engine, inputs, labels)
[rank1]:                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/sky/LLM/run/native.py", line 43, in train_step
[rank1]:     outputs = ds_engine(inputs, labels=labels)
[rank1]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank1]:     ret_val = func(*args, **kwargs)
[rank1]:               ^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 2109, in forward
[rank1]:     loss = self.module(*inputs, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1879, in _call_impl
[rank1]:     return inner()
[rank1]:            ^^^^^^^
[rank1]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1827, in inner
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/transformers/utils/generic.py", line 959, in wrapper
[rank1]:     output = func(self, *args, **kwargs)
[rank1]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 460, in forward
[rank1]:     outputs: BaseModelOutputWithPast = self.model(
[rank1]:                                        ^^^^^^^^^^^
[rank1]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1879, in _call_impl
[rank1]:     return inner()
[rank1]:            ^^^^^^^
[rank1]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1827, in inner
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/transformers/utils/generic.py", line 1083, in wrapper
[rank1]:     outputs = func(self, *args, **kwargs)
[rank1]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 390, in forward
[rank1]:     hidden_states = decoder_layer(
[rank1]:                     ^^^^^^^^^^^^^^
[rank1]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/transformers/modeling_layers.py", line 94, in __call__
[rank1]:     return super().__call__(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1879, in _call_impl
[rank0]:     return inner()
[rank0]:            ^^^^^^^
[rank0]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1827, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 289, in forward
[rank0]:     hidden_states, _ = self.self_attn(
[rank0]:                        ^^^^^^^^^^^^^^^
[rank0]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1879, in _call_impl
[rank0]:     return inner()
[rank0]:            ^^^^^^^
[rank0]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1827, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 260, in forward
[rank0]:     attn_output = self.o_proj(attn_output)
[rank0]:                   ^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1879, in _call_impl
[rank0]:     return inner()
[rank0]:            ^^^^^^^
[rank0]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1827, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 125, in forward
[rank0]:     return F.linear(input, self.weight, self.bias)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: torch.OutOfMemoryError: HIP out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacity of 63.98 GiB of which 6.00 MiB is free. Of the allocated memory 62.98 GiB is allocated by PyTorch, and 35.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
e-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1879, in _call_impl
[rank1]:     return inner()
[rank1]:            ^^^^^^^
[rank1]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1827, in inner
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 289, in forward
[rank1]:     hidden_states, _ = self.self_attn(
[rank1]:                        ^^^^^^^^^^^^^^^
[rank1]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1879, in _call_impl
[rank1]:     return inner()
[rank1]:            ^^^^^^^
[rank1]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1827, in inner
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 260, in forward
[rank1]:     attn_output = self.o_proj(attn_output)
[rank1]:                   ^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1879, in _call_impl
[rank1]:     return inner()
[rank1]:            ^^^^^^^
[rank1]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1827, in inner
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 125, in forward
[rank1]:     return F.linear(input, self.weight, self.bias)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]: torch.OutOfMemoryError: HIP out of memory. Tried to allocate 76.00 MiB. GPU 1 has a total capacity of 63.98 GiB of which 6.00 MiB is free. Of the allocated memory 62.98 GiB is allocated by PyTorch, and 35.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
--------------------------------------------------------------------------
prterun detected that one or more processes exited with non-zero status,
thus causing the job to be terminated. The first process to do so was:

   Process name: [prterun-scc-9911@1,0]
   Exit code:    1
--------------------------------------------------------------------------
