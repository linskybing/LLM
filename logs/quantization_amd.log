/home/sky/miniconda3/envs/deepspeed/bin/deepspeed:4: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
  __import__('pkg_resources').require('deepspeed==0.17.5+a54c3943')
[2025-08-16 01:40:31,406] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-16 01:40:31,484] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-16 01:40:33,767] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-16 01:40:34,450] [WARNING] [runner.py:220:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-08-16 01:40:34,451] [INFO] [runner.py:610:main] cmd = /home/sky/miniconda3/envs/deepspeed/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /home/sky/LLM/run/pretrain.py --deepspeed_config /home/sky/LLM/configs/quantization.json --batch_size 1 --seq_len 350 --total_steps 100
[2025-08-16 01:40:43,076] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-16 01:40:43,109] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-16 01:40:45,127] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-16 01:40:45,747] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-08-16 01:40:45,748] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-08-16 01:40:45,748] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-08-16 01:40:45,748] [INFO] [launch.py:164:main] dist_world_size=2
[2025-08-16 01:40:45,748] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-08-16 01:40:45,749] [INFO] [launch.py:256:main] process 3797892 spawned with command: ['/home/sky/miniconda3/envs/deepspeed/bin/python', '-u', '/home/sky/LLM/run/pretrain.py', '--local_rank=0', '--deepspeed_config', '/home/sky/LLM/configs/quantization.json', '--batch_size', '1', '--seq_len', '350', '--total_steps', '100']
[2025-08-16 01:40:45,749] [INFO] [launch.py:256:main] process 3797893 spawned with command: ['/home/sky/miniconda3/envs/deepspeed/bin/python', '-u', '/home/sky/LLM/run/pretrain.py', '--local_rank=1', '--deepspeed_config', '/home/sky/LLM/configs/quantization.json', '--batch_size', '1', '--seq_len', '350', '--total_steps', '100']
[2025-08-16 01:40:54,517] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-16 01:40:54,549] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-16 01:40:54,593] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-16 01:40:54,625] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-16 01:40:56,496] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-16 01:40:56,566] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[nltk_data] Downloading package words to /home/sky/nltk_data...
[nltk_data]   Package words is already up-to-date!
[nltk_data] Downloading package words to /home/sky/nltk_data...
[nltk_data]   Package words is already up-to-date!
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.56s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.50s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.66s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.55s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.70s/it]
[2025-08-16 01:41:01,771] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.17.5+a54c3943, git-hash=a54c3943, git-branch=master
[2025-08-16 01:41:01,772] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-08-16 01:41:01,936] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.17.5+a54c3943, git-hash=a54c3943, git-branch=master
[2025-08-16 01:41:01,936] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-08-16 01:41:01,936] [INFO] [comm.py:852:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-08-16 01:41:02,815] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 2
[2025-08-16 01:41:02,815] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 2
/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py:1355: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:36.)
  return t.to(
/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py:1355: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:36.)
  return t.to(
[2025-08-16 01:41:05,764] [INFO] [engine.py:1343:_configure_distributed_model] ********** distributed groups summary **********
	 self.dp_world_size=2
	 self.mp_world_size=1
	 self.seq_dp_world_size=2
	 self.sequence_parallel_size=1
***********************************************
[2025-08-16 01:41:05,845] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-08-16 01:41:06,126] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2025-08-16 01:41:06,126] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-08-16 01:41:06,134] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-08-16 01:41:06,134] [INFO] [logging.py:107:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/sky/LLM/run/pretrain.py", line 136, in <module>
[rank1]:     main()
[rank1]:   File "/home/sky/LLM/run/pretrain.py", line 82, in main
[rank1]:     ds_engine, optimizer, _, _ = deepspeed.initialize(
[rank1]:                                  ^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/deepspeed/__init__.py", line 193, in initialize
[rank1]:     engine = DeepSpeedEngine(args=args,
[rank1]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 335, in __init__
[rank1]:     self._configure_optimizer(optimizer, model_parameters)
[rank1]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1456, in _configure_optimizer
[rank1]:     self.optimizer = self._configure_fp16_optimizer(basic_optimizer)
[rank1]:                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1618, in _configure_fp16_optimizer
[rank1]:     optimizer = FP16_Optimizer(
[rank1]:                 ^^^^^^^^^^^^^^^
[rank1]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/deepspeed/runtime/fp16/fused_optimizer.py", line 125, in __init__
[rank1]:     self.initialize_optimizer_states()
[rank1]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/deepspeed/runtime/fp16/fused_optimizer.py", line 132, in initialize_optimizer_states
[rank1]:     self.optimizer.step()
[rank1]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank1]:     out = func(*args, **kwargs)
[rank1]:           ^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py", line 155, in step
[rank1]:     state['exp_avg'] = torch.zeros_like(p.data)
[rank1]:                        ^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]: torch.OutOfMemoryError: HIP out of memory. Tried to allocate 25.10 GiB. GPU 1 has a total capacity of 63.98 GiB of which 350.00 MiB is free. Of the allocated memory 62.76 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/sky/LLM/run/pretrain.py", line 136, in <module>
[rank0]:     main()
[rank0]:   File "/home/sky/LLM/run/pretrain.py", line 82, in main
[rank0]:     ds_engine, optimizer, _, _ = deepspeed.initialize(
[rank0]:                                  ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/deepspeed/__init__.py", line 193, in initialize
[rank0]:     engine = DeepSpeedEngine(args=args,
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 335, in __init__
[rank0]:     self._configure_optimizer(optimizer, model_parameters)
[rank0]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1456, in _configure_optimizer
[rank0]:     self.optimizer = self._configure_fp16_optimizer(basic_optimizer)
[rank0]:                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1618, in _configure_fp16_optimizer
[rank0]:     optimizer = FP16_Optimizer(
[rank0]:                 ^^^^^^^^^^^^^^^
[rank0]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/deepspeed/runtime/fp16/fused_optimizer.py", line 125, in __init__
[rank0]:     self.initialize_optimizer_states()
[rank0]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/deepspeed/runtime/fp16/fused_optimizer.py", line 132, in initialize_optimizer_states
[rank0]:     self.optimizer.step()
[rank0]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank0]:     out = func(*args, **kwargs)
[rank0]:           ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py", line 155, in step
[rank0]:     state['exp_avg'] = torch.zeros_like(p.data)
[rank0]:                        ^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: torch.OutOfMemoryError: HIP out of memory. Tried to allocate 25.10 GiB. GPU 0 has a total capacity of 63.98 GiB of which 344.00 MiB is free. Of the allocated memory 62.76 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W816 01:41:10.107480185 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-08-16 01:41:13,753] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 3797892
[2025-08-16 01:41:13,899] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 3797893
[2025-08-16 01:41:13,899] [ERROR] [launch.py:325:sigkill_handler] ['/home/sky/miniconda3/envs/deepspeed/bin/python', '-u', '/home/sky/LLM/run/pretrain.py', '--local_rank=1', '--deepspeed_config', '/home/sky/LLM/configs/quantization.json', '--batch_size', '1', '--seq_len', '350', '--total_steps', '100'] exits with return code = 1
