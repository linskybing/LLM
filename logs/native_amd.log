bash: /home/sky/miniconda3/envs/deepspeed/lib/libtinfo.so.6: no version information available (required by bash)
bash: /home/sky/miniconda3/envs/deepspeed/lib/libtinfo.so.6: no version information available (required by bash)
[2025-08-22 00:25:35,983] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-22 00:25:36,015] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-22 00:25:36,032] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-22 00:25:36,063] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-22 00:25:37,851] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-22 00:25:37,879] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[nltk_data] Downloading package words to /home/sky/nltk_data...
[nltk_data]   Package words is already up-to-date!
[nltk_data] Downloading package words to /home/sky/nltk_data...
[nltk_data]   Package words is already up-to-date!
/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/transformers/modeling_utils.py:6124: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:36.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/transformers/modeling_utils.py:6124: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:36.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.91s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.34s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.37s/it]
[2025-08-22 00:25:48,160] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.17.5+a54c3943, git-hash=a54c3943, git-branch=master
[2025-08-22 00:25:48,160] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-08-22 00:25:48,160] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 2
[2025-08-22 00:25:48,272] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.17.5+a54c3943, git-hash=a54c3943, git-branch=master
[2025-08-22 00:25:48,272] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-08-22 00:25:48,272] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 2
[2025-08-22 00:25:48,291] [INFO] [engine.py:1343:_configure_distributed_model] ********** distributed groups summary **********
	 self.dp_world_size=2
	 self.mp_world_size=1
	 self.seq_dp_world_size=2
	 self.sequence_parallel_size=1
***********************************************
[2025-08-22 00:25:48,929] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-08-22 00:25:48,930] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-08-22 00:25:48,930] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-08-22 00:25:48,935] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam
[2025-08-22 00:25:48,935] [INFO] [logging.py:107:log_dist] [Rank 0] Creating fp16 unfused optimizer with dynamic loss scale
[2025-08-22 00:25:48,935] [INFO] [unfused_optimizer.py:46:__init__] Fused Lamb Legacy : False 
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/sky/LLM/run/native.py", line 161, in <module>
[rank0]:     main()
[rank0]:   File "/home/sky/LLM/run/native.py", line 106, in main
[rank0]:     ds_engine, optimizer, _, _ = deepspeed.initialize(
[rank0]:                                  ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/deepspeed/__init__.py", line 193, in initialize
[rank0]:     engine = DeepSpeedEngine(args=args,
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 335, in __init__
[rank0]:     self._configure_optimizer(optimizer, model_parameters)
[rank0]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1456, in _configure_optimizer
[rank0]:     self.optimizer = self._configure_fp16_optimizer(basic_optimizer)
[rank0]:                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1645, in _configure_fp16_optimizer
[rank0]:     optimizer = FP16_UnfusedOptimizer(
[rank0]:                 ^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/deepspeed/runtime/fp16/unfused_optimizer.py", line 112, in __init__
[rank0]:     self.initialize_optimizer_states()
[rank0]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/deepspeed/runtime/fp16/unfused_optimizer.py", line 416, in initialize_optimizer_states
[rank0]:     param.grad = torch.zeros(param.size(),
[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: torch.OutOfMemoryError: HIP out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 63.98 GiB of which 10.00 MiB is free. Of the allocated memory 62.82 GiB is allocated by PyTorch, and 705.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/sky/LLM/run/native.py", line 161, in <module>
[rank1]:     main()
[rank1]:   File "/home/sky/LLM/run/native.py", line 106, in main
[rank1]:     ds_engine, optimizer, _, _ = deepspeed.initialize(
[rank1]:                                  ^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/deepspeed/__init__.py", line 193, in initialize
[rank1]:     engine = DeepSpeedEngine(args=args,
[rank1]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 335, in __init__
[rank1]:     self._configure_optimizer(optimizer, model_parameters)
[rank1]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1456, in _configure_optimizer
[rank1]:     self.optimizer = self._configure_fp16_optimizer(basic_optimizer)
[rank1]:                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1645, in _configure_fp16_optimizer
[rank1]:     optimizer = FP16_UnfusedOptimizer(
[rank1]:                 ^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/deepspeed/runtime/fp16/unfused_optimizer.py", line 112, in __init__
[rank1]:     self.initialize_optimizer_states()
[rank1]:   File "/home/sky/miniconda3/envs/deepspeed/lib/python3.11/site-packages/deepspeed/runtime/fp16/unfused_optimizer.py", line 416, in initialize_optimizer_states
[rank1]:     param.grad = torch.zeros(param.size(),
[rank1]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]: torch.OutOfMemoryError: HIP out of memory. Tried to allocate 64.00 MiB. GPU 1 has a total capacity of 63.98 GiB of which 74.00 MiB is free. Of the allocated memory 62.76 GiB is allocated by PyTorch, and 705.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W822 00:25:50.495112653 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
--------------------------------------------------------------------------
prterun detected that one or more processes exited with non-zero status,
thus causing the job to be terminated. The first process to do so was:

   Process name: [prterun-scc-4123871@1,0]
   Exit code:    1
--------------------------------------------------------------------------
